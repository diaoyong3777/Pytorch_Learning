这是 PyTorch `torch.nn` 模块的官方文档结构！下面详细解释每个部分的作用：
https://docs.pytorch.org/docs/stable/nn.html

## torch.nn 模块结构详解

### **Containers（容器）**
```python
nn.Module        # 所有神经网络模块的基类
nn.Sequential    # 顺序容器，按顺序执行模块
nn.ModuleList    # 模块列表，像Python列表一样存储模块
nn.ModuleDict    # 模块字典，像Python字典一样存储模块
nn.ParameterList # 参数列表
nn.ParameterDict # 参数字典
```
**作用**：组织和管理神经网络模块的容器类

---

### **Convolution Layers（卷积层）**
```python
nn.Conv1d        # 一维卷积，用于时序数据
nn.Conv2d        # 二维卷积，用于图像处理 ✓
nn.Conv3d        # 三维卷积，用于视频数据
nn.ConvTranspose1d/2d/3d  # 转置卷积（反卷积）
```
**作用**：提取空间特征，是CNN的核心

---

### **Pooling layers（池化层）**
```python
nn.MaxPool1d/2d/3d     # 最大池化
nn.AvgPool1d/2d/3d     # 平均池化
nn.AdaptiveMaxPool1d/2d/3d  # 自适应池化
nn.AdaptiveAvgPool1d/2d/3d
```
**作用**：降维，减少计算量，增强特征不变性

---

### **Padding Layers（填充层）**
```python
nn.ReflectionPad2d    # 反射填充
nn.ReplicationPad2d   # 复制填充
nn.ZeroPad2d          # 零填充
nn.ConstantPad2d      # 常数填充
```
**作用**：在图像边缘添加像素，控制输出尺寸

---

### **Non-linear Activations（非线性激活函数）**
#### **加权求和类**
```python
nn.ReLU          # 修正线性单元 ✓
nn.LeakyReLU     # 带泄露的ReLU
nn.PReLU         # 参数化ReLU
nn.Sigmoid       # S型函数
nn.Tanh          # 双曲正切
```

#### **其他非线性**
```python
nn.Softmax       # 软最大化，用于多分类
nn.Softplus      # 软加函数
nn.Softsign      # 软符号函数
nn.GELU          # 高斯误差线性单元
```
**作用**：引入非线性，使网络能够学习复杂模式

---

### **Normalization Layers（归一化层）**
```python
nn.BatchNorm1d/2d/3d    # 批归一化 ✓
nn.LayerNorm            # 层归一化
nn.InstanceNorm1d/2d/3d # 实例归一化
nn.GroupNorm            # 组归一化
```
**作用**：加速训练，提高模型稳定性

---

### **Recurrent Layers（循环层）**
```python
nn.RNN           # 循环神经网络
nn.LSTM          # 长短时记忆网络 ✓
nn.GRU           # 门控循环单元
nn.RNNCell       # RNN单元
nn.LSTMCell      # LSTM单元
nn.GRUCell       # GRU单元
```
**作用**：处理序列数据，如文本、语音

---

### **Transformer Layers（Transformer层）**
```python
nn.Transformer          # Transformer模型
nn.TransformerEncoder   # Transformer编码器
nn.TransformerDecoder   # Transformer解码器
nn.MultiheadAttention   # 多头注意力机制 ✓
```
**作用**：处理序列数据的现代架构

---

### **Linear Layers（线性层）**
```python
nn.Linear        # 全连接层 ✓
nn.Bilinear      # 双线性层
```
**作用**：执行线性变换，用于分类和回归

---

### **Dropout Layers（丢弃层）**
```python
nn.Dropout       # 标准丢弃
nn.Dropout2d     # 2D丢弃
nn.Dropout3d     # 3D丢弃
nn.AlphaDropout  # Alpha丢弃
```
**作用**：防止过拟合，提高泛化能力

---

### **Sparse Layers（稀疏层）**
```python
nn.Embedding     # 词嵌入层 ✓
nn.EmbeddingBag  # 词嵌入包
```
**作用**：处理离散特征，如自然语言处理

---

### **Distance Functions（距离函数）**
```python
nn.CosineSimilarity  # 余弦相似度
nn.PairwiseDistance  # 成对距离
```
**作用**：计算向量间的相似度或距离

---

### **Loss Functions（损失函数）**
```python
nn.MSELoss           # 均方误差损失
nn.CrossEntropyLoss  # 交叉熵损失 ✓
nn.NLLLoss           # 负对数似然损失
nn.BCELoss           # 二值交叉熵损失
nn.L1Loss            # L1损失（绝对误差）
nn.SmoothL1Loss      # 平滑L1损失
```
**作用**：衡量模型预测与真实值的差距

---

### **Vision Layers（视觉层）**
```python
nn.PixelShuffle     # 像素重排
nn.Upsample         # 上采样
nn.UpsamplingNearest2d  # 最近邻上采样
nn.UpsamplingBilinear2d # 双线性上采样
```
**作用**：图像上采样和分辨率调整

---

### **Shuffle Layers（重排层）**
```python
nn.ChannelShuffle   # 通道重排
```
**作用**：重新排列通道顺序

---

### **DataParallel Layers（数据并行层）**
```python
nn.DataParallel     # 数据并行
nn.DistributedDataParallel  # 分布式数据并行
```
**作用**：多GPU训练支持

---

### **Utilities（工具函数）**
```python
nn.utils.spectral_norm    # 谱归一化
nn.utils.weight_norm      # 权重归一化
nn.utils.clip_grad_norm_  # 梯度裁剪
nn.utils.rnn.pack_padded_sequence  # 打包填充序列
```
**作用**：提供各种实用工具函数

---

### **Quantized Functions（量化函数）**
```python
nn.quantized.Conv2d    # 量化卷积
nn.quantized.Linear    # 量化线性层
```
**作用**：模型量化，减少内存和计算需求

---

### **Lazy Modules Initialization（惰性模块初始化）**
```python
nn.LazyLinear      # 惰性线性层
nn.LazyConv2d      # 惰性卷积层
```
**作用**：延迟参数初始化，直到第一次前向传播

## 在实际项目中的典型使用

```python
import torch.nn as nn

# 典型的CNN模型结构
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        # 卷积层
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        # 激活函数
        self.relu = nn.ReLU()
        # 池化层
        self.pool = nn.MaxPool2d(2)
        # 归一化层
        self.bn = nn.BatchNorm2d(64)
        # 丢弃层
        self.dropout = nn.Dropout(0.5)
        # 线性层
        self.fc = nn.Linear(64 * 16 * 16, 10)
        # 损失函数
        self.criterion = nn.CrossEntropyLoss()
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.dropout(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
```

这个结构组织让 PyTorch 非常模块化和易用，你可以像搭积木一样构建复杂的神经网络！